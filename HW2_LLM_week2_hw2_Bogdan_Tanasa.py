# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wZSS2emdq0cflJbHJEKfI6VRi5OhqAqS
"""

# Problem 1.Compute the SVD (Singular Value Decomposition) (3 separate matrices U,Sigma,V) of the following matrix A.


import numpy as np

# Input the original matrix A
A = np.array([[5, 7, 21, 32],
              [0, 3, 43, 24],
              [2, 36, 95, 84],
              [92, 37, 57, 64]])

# Compute the SVD of matrix A
U, Sigma, Vt = np.linalg.svd(A, full_matrices=False)

# Print the results
print("Matrix U:")
print(U)
print("\nSingular values (Sigma):")
print(Sigma)
print("\nMatrix V^T:")
print(Vt)

# Create a diagonal Sigma matrix
Sigma_Matrix = np.diag(Sigma)
print("Sigma Matrix:")
print(Sigma_Matrix)

print("The number of rows and columns in the matrix U:")
print(U.shape)

print("The number of rows and columns in the matrix Sigma_Matrix:")
print(Sigma_Matrix.shape)

print("The number of rows and columns in the matrix Vt:")
print(Vt.shape)

# Recompute the original matrix A:
A_recomputed = np.dot(U, np.dot(Sigma_Matrix, Vt))
print("The recomputed A matrix:")
print(A_recomputed)

# Check whether the recomputed A matrix is identical to the original matrix A:
are_equal = np.allclose(A, A_recomputed)
print("Are the original and recomputed matrices equal?", are_equal)



# Problem 2

# Start with Matrix A created in the previous problem, and make the first row of matrix â€˜Aâ€™ be the sum of the 2nd and the 3rd rows.
# Make the last row of Matrix A same as the 2nd row.
# Compute the SVD of this new matrix.
# Why the last 2 values of the â€˜Sigmaâ€™ matrix values are zeros ?



import numpy as np

# Input the original matrix A
A = np.array([[5, 7, 21, 32],
              [0, 3, 43, 24],
              [2, 36, 95, 84],
              [92, 37, 57, 64]])

print("the original matrix A :")
print(A)
print(A.shape)

A[0] = A[1] + A[2]
A[-1] = A[1]

print("the modified matrix A :")
print(A)
print(A.shape)

# Calculate the SVD of matrix A
U, S, VT = np.linalg.svd(A)

# Print the results
print("Matrix U:")
print("It contains the left singular vectors.")
print(U)

print("\n Matrix Sigma of the Singular Values (as a diagonal matrix): ")
print(np.diag(S))

print("\n Matrix Sigma of the Singular Values (as a diagonal matrix with integer values): ")
print(np.diag(S).astype(int))

print("\nMatrix VT:")
print(" It contains the right singular vectors. ")
print(VT)

# Why the last 2 values of the â€˜Sigmaâ€™ matrix values are zeros

print(
'''
Let's recall that the singular values in the matrix Sigma represent the amount of variance captured by each dimension.

In our modified matrix A, the operations we performed resulted in linear dependencies among the rows:

* The first row is set to the sum of the second and third rows.
* The last row is set to be identical to the second row.

Certain dimensions do not capture any variance, because the corresponding rows/columns are linearly dependent,
and their singular values will be zero.

''')



# Problem 3
# Compute the SVD (Singular Value Decomposition) (3 separate matrices U, Sigma, V) of the following 2 rectangular matrices.
# Verify that by multiplying (Matrix multiplication) the 3 SVD matrices will result in the original matrix

# In order to solve this problem, we consider the following steps :

# Input and print the matrix
A = np.array([[3, 1, 1],
              [-1, 3, 1]])
print(A)

# In a simple form the code is the following :

U, Sigma, Vt = np.linalg.svd(A)

# Display the results
print("\nMatrix U:")
print(U)

print("\nThe singular values:")
print(Sigma)

print("\nMatrix V^T:")
print(Vt)

# In order to be be able to recompute the original matrix, we have to expand the Sigma matrix :

# Construct Sigma matrix
Sigma_Matrix = np.zeros((U.shape[1], Vt.shape[0]))
np.fill_diagonal(Sigma_Matrix, Sigma)

print("\n The matrix Sigma:")
print(Sigma_Matrix)

# Recompute the original matrix A:
A_recomputed = np.dot(U, np.dot(Sigma_Matrix, Vt))
print("The recomputed A matrix:")
print(A_recomputed)

# Check whether the recomputed A matrix is identical to the original matrix A:
are_equal = np.allclose(A.astype(int), A_recomputed)
print("Are the original and recomputed matrices equal?", are_equal)

# In order to solve this problem, we consider the following steps :

# Input and print the matrix
A = np.array([[1, 1],
              [0, 1],
              [-1,1]])
print(A)

# In a simple form the code is the following :

U, Sigma, Vt = np.linalg.svd(A)

# Display the results
print("\nMatrix U:")
print(U)

print("\nThe singular values:")
print(Sigma)

print("\nMatrix V^T:")
print(Vt)

# In order to be be able to recompute the original matrix, we have to expand the Sigma matrix :

# Construct Sigma matrix
Sigma_Matrix = np.zeros((U.shape[1], Vt.shape[0]))
np.fill_diagonal(Sigma_Matrix, Sigma)

print("\n The matrix Sigma:")
print(Sigma_Matrix)

# Recompute the original matrix A:
A_recomputed = np.dot(U, np.dot(Sigma_Matrix, Vt))
print("The recomputed A matrix:")
print(A_recomputed)

# Check whether the recomputed A matrix is identical to the original matrix A:
are_equal = np.allclose(A.astype(int), A_recomputed)
print("Are the original and recomputed matrices equal?", are_equal)



# Problem 4
# ğ¿ğ‘’ğ‘¡ âˆ¶
# ğ´ ğ‘ğ‘’ ğ‘ğ‘› (ğ‘š ğ‘¥ ğ‘›) ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥ ğ‘¤ğ‘–ğ‘¡â„ ğ‘Ÿğ‘ğ‘›ğ‘˜ ğ‘Ÿ,
# ğ´ = ğ‘ˆ . ğ‘†. ğ‘‰ ğ‘ğ‘’ ğ‘¡â„ğ‘’ ğ‘ ğ‘–ğ‘›ğ‘”ğ‘¢ğ‘™ğ‘ğ‘Ÿ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’ ğ‘‘ğ‘’ğ‘ğ‘œğ‘šğ‘ğ‘œğ‘ ğ‘–ğ‘¡ğ‘–ğ‘œğ‘›.
# ğ‘Šâ„ğ‘’ğ‘Ÿğ‘’ ğ‘ˆ ğ‘–ğ‘  ğ‘ğ‘› (ğ‘š ğ‘¥ ğ‘Ÿ) ğ‘ğ‘œğ‘™ğ‘¢ğ‘šğ‘› âˆ’ ğ‘œğ‘Ÿğ‘¡â„ğ‘œğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥
# ğ‘† ğ‘–ğ‘  ğ‘ ğ‘‘ğ‘–ğ‘ğ‘”ğ‘œğ‘›ğ‘ğ‘™ (ğ‘Ÿ ğ‘¥ ğ‘Ÿ) ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥,
# ğ‘ğ‘›ğ‘‘ ğ‘‰ ğ‘–ğ‘  ğ‘ğ‘› (ğ‘Ÿ ğ‘¥ ğ‘›) ğ‘Ÿğ‘œğ‘¤ âˆ’ ğ‘œğ‘Ÿğ‘¡â„ğ‘œğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™ ğ‘šğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥.
# Prove that ğ‘‰ = ğ‘† âˆ’1 . ğ‘ˆ ğ‘‡ . ğ´

print('''Proof :

1. we start with the SVD equation: A = U â‹… S â‹… V

2. we multiply both sides by UT from the left: UT â‹… A = UT â‹… U â‹… S â‹… V

   UT â‹… U = Ir aka the identity matrix, because U is defined as a column-orthonormal matrix in a given SVD.

   the expression above becomes :

   UT â‹… A = S â‹… V

3. we multiply both sides by Sâˆ’1 from the left:

   Sâˆ’1 â‹… UT â‹… A = Sâˆ’1 â‹… S â‹… V

   As Sâˆ’1 â‹… S = Ir, the result is : V = Sâˆ’1 â‹… UT â‹… A

4. qed

''')

# to code the algorithm described above :

A = np.array([[5, 7, 21, 32],
              [0, 3, 43, 24],
              [2, 36, 95, 84],
              [92, 37, 57, 64]])

# Step 1: Perform SVD on matrix A
U, S, Vt = np.linalg.svd(A)
print('matrix U \n', U)
print('singular values S \n', S)
print('matrix Vt \n', Vt)

# Step 2: Compute UT â‹… A
UT_A = np.dot(U.T, A)
print('matrix UT_A \n', UT_A)

# Step 3: Compute S inverse (for non-zero singular values)
# Create a diagonal matrix from the singular values
S_diag = np.diag(S)
# Invert the diagonal matrix by inverting the singular values
S_inv = np.diag(1 / S)
print('matrix S_inv \n', S_inv)

# Step 4: Compute Sâˆ’1 â‹… UT â‹… A
S_inv_UT_A = np.dot(S_inv, UT_A)
print('matrix S_inv_UT_A \n', S_inv_UT_A)

# Step 5: V is the result (from the equation: V = Sâˆ’1 â‹… UT â‹… A)
V = S_inv_UT_A
print('matrix V \n', V)



# Problem 5
# Using the Singular Value Decomposition (SVD) principals, perform the Latent Semantics Analysis (LSA) of the following 6 documents.
# Divide these documents into 2 different topics.
#  'runs',
#  'runs pitcher innings',
#  'pitcher',
#  'touchdown',
#  'quarterback',
#  'touchdown quarterback'

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

np.set_printoptions(precision=4,suppress=True)

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, cosine_distances

vectorizer1 = CountVectorizer()

corpus = [
          'runs',
          'runs pitcher innings',
          'pitcher',
          'touchdown',
          'quarterback',
          'touchdown quarterback'
         ]

print(corpus)

# A solution is provided by the use of TfidfVectorizer

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity, cosine_distances


# Step 1: Vectorization using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

# Step 2: Convert the sparse matrix to a dense format
X_dense = X.todense()
print(X_dense)

# Step 3: Create a heatmap for the dense matrix
plt.figure(figsize=(4, 4))
sns.heatmap(X_dense, annot=True,
                     cmap='coolwarm',
                     xticklabels = vectorizer.get_feature_names_out(),
                     yticklabels=corpus)
plt.title('TF-IDF Matrix Heatmap')
plt.xlabel('Terms')
plt.ylabel('Documents')
plt.show()

# Step 4: Compute cosine similarity
cosine_sim = cosine_similarity(X)
print("Cosine Similarity Matrix:")
print(cosine_sim)

# Step 5: Create a heatmap for cosine similarity
plt.figure(figsize=(4, 4))
sns.heatmap(cosine_sim, annot=True,
                        cmap='coolwarm',
                        xticklabels=corpus,
                        yticklabels=corpus)
plt.title('Cosine Similarity Heatmap')
plt.xlabel('Documents')
plt.ylabel('Documents')
plt.show()

# Step 6: Compute cosine distance
cosine_dist = cosine_distances(X)
print("\nCosine Distance Matrix:")
print(cosine_dist)

# Step 7: Create a heatmap for cosine distance
plt.figure(figsize=(4, 4))
sns.heatmap(cosine_dist, annot=True,
                         cmap='coolwarm',
                         xticklabels=corpus,
                         yticklabels=corpus)
plt.title('Cosine Distance Heatmap')
plt.xlabel('Documents')
plt.ylabel('Documents')
plt.show()

# A second solution is provided by the following classes : CountVectorizer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD

# Step 1: Vectorization using CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)
# print(X)

# Step 2: Convert the sparse matrix to a dense format
X_dense = X.todense()
print(X_dense)

# Step 3: Create a heatmap for the dense matrix
plt.figure(figsize=(4, 4))
sns.heatmap(X_dense, annot=True,
                     cmap='coolwarm',
                     xticklabels = vectorizer.get_feature_names_out(),
                     yticklabels=corpus)
plt.title('CountVectorizer Matrix Heatmap')
plt.xlabel('Terms')
plt.ylabel('Documents')
plt.show()

# Step 4: Compute cosine similarity
cosine_sim = cosine_similarity(X)
print("Cosine Similarity Matrix:")
print(cosine_sim)

# Step 5: Create a heatmap for cosine similarity
plt.figure(figsize=(4, 4))
sns.heatmap(cosine_sim, annot=True,
                        cmap='coolwarm',
                        xticklabels=corpus,
                        yticklabels=corpus)
plt.title('Cosine Similarity Heatmap')
plt.xlabel('Documents')
plt.ylabel('Documents')
plt.show()

# Step 6: Compute cosine distance
cosine_dist = cosine_distances(X)
print("\nCosine Distance Matrix:")
print(cosine_dist)

# Step 7: Create a heatmap for cosine distance
plt.figure(figsize=(4, 4))
sns.heatmap(cosine_dist, annot=True,
                         cmap='coolwarm',
                         xticklabels=corpus,
                         yticklabels=corpus)
plt.title('Cosine Distance Heatmap')
plt.xlabel('Documents')
plt.ylabel('Documents')
plt.show()



# 2. Compute the â€˜Document Term Matrixâ€™ (DTM) and â€˜Term (Word) Document Matrixâ€™ (TDM) using
# the documentâ€™s vectors.

# Step 1: Vectorization using CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(corpus)

# Step 2: Convert the sparse matrix to a dense array
dtm = X.toarray()

# Step 3 : Compute Document Term Matrix (DTM)
dtm_df = pd.DataFrame(dtm, columns = vectorizer.get_feature_names_out(),
                            index = [f'Doc {i+1}' for i in range(len(corpus))]
                      )

# Step 4: Compute Term Document Matrix (TDM)
tdm_df = dtm_df.T

# Step 5: Display the matrices DTM and TDM
print("Document Term Matrix (DTM):")
print(dtm_df)

print("\nTerm Document Matrix (TDM):")
print(tdm_df)


# 3. Compute the â€˜Singular Value Decompositionâ€™ (SVD) matrix for both DTM (Document Term Matrix) &
# TDM (Term Document Matrix) computed in the previous step. Using only the first 2
# eigen values from the SVD and the SK Learn TruncatedSVD function, compute the Truncated DTM and TDM.


svd = TruncatedSVD(n_components=2)

# Perform SVD on TDM
tdm_svd = svd.fit_transform(tdm_df)
print("SVD of TDM:")
print(tdm_svd)

# Perform SVD on DTM :
dtm_svd = svd.fit_transform(dtm_df)
print("SVD of DTM:")
print(dtm_svd)

# What are the topics on which LSA has divided these set of documents?
# Plot all the 6 Truncated vectors with the following specifications.
# â€¢ X-axis: Truncated DTM/Topic-1 values
# â€¢ Y-axis: Truncated DTM/Topic-2 values
# Compute the â€˜cosine similarityâ€™ and â€˜cosine distanceâ€™ using the Truncated SVD.

# Extract Topic-1 and Topic-2 values
topic_1 = dtm_svd[:, 0]
topic_2 = dtm_svd[:, 1]

# Plotting the topics

plt.figure(figsize=(4, 4))
plt.scatter(topic_1, topic_2, color='blue')
for i, txt in enumerate(range(len(corpus))):
     plt.annotate(f'Doc {txt+1}',
                 (topic_1[i], topic_2[i]),
                 fontsize=9)
plt.title('Truncated DTM Topics')
plt.xlabel('Topic 1 (Truncated DTM)')
plt.ylabel('Topic 2 (Truncated DTM)')
plt.grid()
plt.axhline(0, color='grey', lw=0.5, ls='--')
plt.axvline(0, color='grey', lw=0.5, ls='--')
plt.show()

# Displaying the encoded topics :

dtm_df = pd.DataFrame(dtm_svd, columns=["Topic-1", "Topic-2"])
dtm_df ["Corpus"] = corpus
dtm_df.head()
print(dtm_df.head(10))



# Compute the â€˜cosine similarityâ€™ and â€˜cosine distanceâ€™ using the Truncated SVD.

dtm_svd_cos_sim = cosine_similarity(dtm_svd)
print("the cosine similarity is :")
print(dtm_svd_cos_sim.astype(int))

# Compute the â€˜cosine similarityâ€™ and â€˜cosine distanceâ€™ using the Truncated SVD.

dtm_svd_cos_dis = 1 - dtm_svd_cos_sim
print("the cosine distance is :")
print(dtm_svd_cos_dis.astype(int))

# As we can see in the matrices above, the documents 0, 1 and 2 describe Topic 1,
# while the documents 3, 4, 5 describe Topic 2.
# The word "runs" is very relevant for Topic 1, 
# while the word "touchdown quarterback" is very relevant for Topic 2.