{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7abdec1c-2afb-4ecd-8632-a52a5044f017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2c93f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size = 1\n",
      "Index \t Input \t Label\n",
      "0 \t data \t science\n",
      "1 \t science \t data\n",
      "2 \t science \t professionals\n",
      "3 \t professionals \t science\n",
      "4 \t professionals \t have\n",
      "5 \t have \t professionals\n",
      "6 \t have \t promising\n",
      "7 \t promising \t have\n",
      "8 \t promising \t career\n",
      "9 \t career \t promising\n",
      "10 \t career \t path\n",
      "11 \t path \t career\n",
      "\n",
      "Number of skip-gram entries for window size 1: 12\n",
      "Window Size = 2\n",
      "Index \t Input \t Label\n",
      "0 \t data \t science\n",
      "1 \t data \t professionals\n",
      "2 \t science \t data\n",
      "3 \t science \t professionals\n",
      "4 \t science \t have\n",
      "5 \t professionals \t science\n",
      "6 \t professionals \t have\n",
      "7 \t professionals \t data\n",
      "8 \t professionals \t promising\n",
      "9 \t have \t professionals\n",
      "10 \t have \t promising\n",
      "11 \t have \t science\n",
      "12 \t have \t career\n",
      "13 \t promising \t have\n",
      "14 \t promising \t career\n",
      "15 \t promising \t professionals\n",
      "16 \t promising \t path\n",
      "17 \t career \t promising\n",
      "18 \t career \t path\n",
      "19 \t career \t have\n",
      "20 \t path \t career\n",
      "21 \t path \t promising\n",
      "\n",
      "Number of skip-gram entries for window size 2: 22\n",
      "Window Size = 3\n",
      "Index \t Input \t Label\n",
      "0 \t data \t science\n",
      "1 \t data \t professionals\n",
      "2 \t data \t have\n",
      "3 \t science \t data\n",
      "4 \t science \t professionals\n",
      "5 \t science \t have\n",
      "6 \t science \t promising\n",
      "7 \t professionals \t science\n",
      "8 \t professionals \t have\n",
      "9 \t professionals \t data\n",
      "10 \t professionals \t promising\n",
      "11 \t professionals \t career\n",
      "12 \t have \t professionals\n",
      "13 \t have \t promising\n",
      "14 \t have \t science\n",
      "15 \t have \t career\n",
      "16 \t have \t data\n",
      "17 \t have \t path\n",
      "18 \t promising \t have\n",
      "19 \t promising \t career\n",
      "20 \t promising \t professionals\n",
      "21 \t promising \t path\n",
      "22 \t promising \t science\n",
      "23 \t career \t promising\n",
      "24 \t career \t path\n",
      "25 \t career \t have\n",
      "26 \t career \t professionals\n",
      "27 \t path \t career\n",
      "28 \t path \t promising\n",
      "29 \t path \t have\n",
      "\n",
      "Number of skip-gram entries for window size 3: 30\n"
     ]
    }
   ],
   "source": [
    "def skip_ngrams(text, max_window_size=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate skip-grams for the given text within the specified window sizes.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to generate skip-grams from.\n",
    "    - max_window_size (int): The maximum window size to consider for skip-grams.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples containing (input_word, label_word).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1 : Split the text into separate words\n",
    "    words = text.split()\n",
    "    skip_grams = []\n",
    "\n",
    "    # Step 2 : Iterate over each word with its index\n",
    "    \n",
    "    for index, word in enumerate(words):\n",
    "    \n",
    "        # Create pairs with neighboring words within the window sizes\n",
    "        \n",
    "        for window_size in range(1, max_window_size + 1):\n",
    "            # Locate and get the left neighbor\n",
    "            if index - window_size >= 0:\n",
    "                skip_grams.append((word, words[index - window_size]))\n",
    "            # Locate and get the right neighbor\n",
    "            if index + window_size < len(words):\n",
    "                skip_grams.append((word, words[index + window_size]))\n",
    "\n",
    "    return skip_grams\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    text = \"data science professionals have promising career path\"\n",
    "    window_sizes = [1, 2, 3]\n",
    "\n",
    "    for window in window_sizes:\n",
    "        print(f\"Window Size = {window}\")\n",
    "        skip_grams = skip_ngrams(text, max_window_size=window)\n",
    "        \n",
    "        # Print the header\n",
    "        print(f\"Index \\t Input \\t Label\")\n",
    "       \n",
    "        # Print each skip-gram pair with its index\n",
    "        for i, (input_word, label_word) in enumerate(skip_grams):\n",
    "            print(f\"{i} \\t {input_word} \\t {label_word}\")\n",
    "        \n",
    "        # Print the total number of entries\n",
    "        total_entries = len(skip_grams)\n",
    "        print(f\"\\nNumber of skip-gram entries for window size {window}: {total_entries}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6180f30-9794-4511-8805-8bf778413bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc30310-e768-4bbf-9259-babb1bb635da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window Size = 2\n",
      "Index \t Input \t Label\n",
      "0 \t data \t science\n",
      "1 \t data \t professionals\n",
      "2 \t science \t data\n",
      "3 \t science \t professionals\n",
      "4 \t science \t have\n",
      "5 \t professionals \t science\n",
      "6 \t professionals \t have\n",
      "7 \t professionals \t data\n",
      "8 \t professionals \t promising\n",
      "9 \t have \t professionals\n",
      "10 \t have \t promising\n",
      "11 \t have \t science\n",
      "12 \t have \t career\n",
      "13 \t promising \t have\n",
      "14 \t promising \t career\n",
      "15 \t promising \t professionals\n",
      "16 \t promising \t path\n",
      "17 \t career \t promising\n",
      "18 \t career \t path\n",
      "19 \t career \t have\n",
      "20 \t path \t career\n",
      "21 \t path \t promising\n",
      "\n",
      "Number of skip-gram entries for window size 3: 22\n",
      "Unique Words: ['career', 'data', 'have', 'path', 'professionals', 'promising', 'science']\n",
      "\n",
      "Input One-Hot Encoded:\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 0]]\n",
      "\n",
      "Output One-Hot Encoded:\n",
      "[[0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]]\n",
      "\n",
      "Word to Index Mapping:\n",
      "career: 0\n",
      "data: 1\n",
      "have: 2\n",
      "path: 3\n",
      "professionals: 4\n",
      "promising: 5\n",
      "science: 6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def skip_ngrams(text, max_window_size=2, structured_output=False):\n",
    "    \"\"\"\n",
    "    Generate skip-grams for the given text within the specified window sizes.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to generate skip-grams from.\n",
    "    - max_window_size (int): The maximum window size to consider for skip-grams.\n",
    "    - structured_output (bool): If True, return a list of dictionaries instead of tuples.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples or dictionaries containing (input_word, label_word).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Split the text into separate words\n",
    "    \n",
    "    words = text.split()\n",
    "    skip_grams = []\n",
    "\n",
    "    # Step 2: Iterate over each word with its index\n",
    "    \n",
    "    for index, word in enumerate(words):\n",
    "    \n",
    "        # Create pairs with neighboring words within the window sizes\n",
    "        for window_size in range(1, max_window_size + 1):\n",
    "            \n",
    "            # Locate and get the left neighbor\n",
    "            if index - window_size >= 0:\n",
    "                if structured_output:\n",
    "                    skip_grams.append({\"input_word\": word, \"label_word\": words[index - window_size]})\n",
    "                else:\n",
    "                    skip_grams.append((word, words[index - window_size]))\n",
    "\n",
    "            # Locate and get the right neighbor\n",
    "            if index + window_size < len(words):\n",
    "                if structured_output:\n",
    "                    skip_grams.append({\"input_word\": word, \"label_word\": words[index + window_size]})\n",
    "                else:\n",
    "                    skip_grams.append((word, words[index + window_size]))\n",
    "\n",
    "    return skip_grams\n",
    "\n",
    "def one_hot_encode(skip_grams, unique_words):\n",
    "    \"\"\"\n",
    "    Generate one-hot encoded input and output data from skip-grams.\n",
    "\n",
    "    Parameters:\n",
    "    - skip_grams (list): The list of skip-grams.\n",
    "    - unique_words (list): The list of unique words in the text.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of numpy arrays: (input_hot, output_hot)\n",
    "    \"\"\"\n",
    "    \n",
    "    word_to_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    \n",
    "    input_hot = np.zeros((len(skip_grams), len(unique_words)), dtype=int)\n",
    "    output_hot = np.zeros((len(skip_grams), len(unique_words)), dtype=int)\n",
    "\n",
    "    for i, (input_word, label_word) in enumerate(skip_grams):\n",
    "        input_index = word_to_index[input_word]\n",
    "        label_index = word_to_index[label_word]\n",
    "\n",
    "        input_hot[i, input_index] = 1\n",
    "        output_hot[i, label_index] = 1\n",
    "\n",
    "    return input_hot, output_hot, word_to_index\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"data science professionals have promising career path\"\n",
    "    window_size = 2\n",
    "\n",
    "    # Generate skip-grams\n",
    "    skip_grams = skip_ngrams(text, max_window_size=window_size)\n",
    "\n",
    "    # Define unique words in the desired order\n",
    "    unique_words = ['career', 'data', 'have', 'path', 'professionals', 'promising', 'science']\n",
    "\n",
    "    print(f\"Window Size = {window_size}\")\n",
    "    \n",
    "    print(f\"Index \\t Input \\t Label\")\n",
    "    for i, (input_word, label_word) in enumerate(skip_grams):\n",
    "            print(f\"{i} \\t {input_word} \\t {label_word}\")\n",
    "        \n",
    "    # Print the total number of entries\n",
    "    total_entries = len(skip_grams)\n",
    "    print(f\"\\nNumber of skip-gram entries for window size {window}: {total_entries}\")\n",
    "    \n",
    "    # Generate one-hot encoded data\n",
    "    input_hot, output_hot, word_to_index = one_hot_encode(skip_grams, unique_words)\n",
    "\n",
    "\n",
    "    print(\"Unique Words:\", unique_words)\n",
    "    print(\"\\nInput One-Hot Encoded:\")\n",
    "    print(input_hot)\n",
    "    print(\"\\nOutput One-Hot Encoded:\")\n",
    "    print(output_hot)\n",
    "\n",
    "    # Print the mapping of words to their indices\n",
    "    print(\"\\nWord to Index Mapping:\")\n",
    "    for word in unique_words:\n",
    "        print(f\"{word}: {word_to_index[word]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2059d017-50c2-456c-a301-efccea645b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
